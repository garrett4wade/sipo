base:
  num_train_envs: 256
  n_rollout_threads: 8
  num_eval_envs: 16
  eval_episodes: 100
  eval_interval: 50
  n_eval_rollout_threads: 4
  num_env_splits: 2

  episode_length: 25
  num_env_steps: 8.0e+5
  ppo_epoch: 5

  use_wandb: False
  wandb_project: mpe
  wandb_group: check

  # seed should not be changed throughout iterations
  seed: 7
  seed_specify: True

  # diversity algorithm parameters
  rbf_gamma: 5.0e-3  # if changed, threshold should be probably changed together
  threshold_eps: 3.1
  warmup_fraction: 0.6

  n_iterations: 4
  ll_max: 10.0
  lagrangian_lr: 0.1
  intrinsic_reward_scaling: 1.0e-1
  # archive_policy_dirs:
  #   - results/mpe/check/1/run1/iter0/models/model.pt
  #   - results/mpe/check/1/run1/iter1/models/model.pt
  #   - results/mpe/check/1/run1/iter2/models/model.pt
  #   - results/mpe/check/1/run1/iter3/models/model.pt
  # archive_traj_dirs:
  #   - results/mpe/check/1/run1/iter0/models/data.traj
  #   - results/mpe/check/1/run1/iter1/models/data.traj
  #   - results/mpe/check/1/run1/iter2/models/data.traj
  #   - results/mpe/check/1/run1/iter3/models/data.traj

policy:
  type: actor-critic
  args:
    hidden_dim: 64
    num_dense_layers: 2
    num_rnn_layers: 0

environment:
  type: mpe
  args:
    base:
      scenario_name: simple_more_easy
