base:
  num_train_envs: 128
  n_rollout_threads: 32
  num_eval_envs: 16
  eval_episode_length: 1000
  eval_interval: 200
  n_eval_rollout_threads: 8
  num_env_splits: 2

  episode_length: 10
  num_env_steps: 1.0e+7
  ppo_epoch: 10

  log_interval: 10

  use_wandb: False
  wandb_project: fast_mappo_ma_mujoco
  wandb_group: halfcheetah

  # seed should not be changed throughout iterations
  seed: 1
  seed_specify: True

  # diversity algorithm parameters
  rbf_gamma: 5.0e-3 # if changed, threshold should be probably changed together
  threshold_eps: 9.27
  warmup_fraction: 0.05
  rkhs_action: False

  n_iterations: 1
  ll_max: 10.0
  lagrangian_lr: 0.0
  intrinsic_reward_scaling: 1.0e-3
  inherit_policy: False
  # archive_policy_dirs:
  #   - /localdata/stuff/fw/fast_mappo/results/football/check/1/run1/iter0/models/model.pt
  #   - /localdata/stuff/fw/fast_mappo/results/football/check/1/run8/iter1/models/model.pt
  # archive_traj_dirs:
  #   - /localdata/stuff/fw/fast_mappo/results/football/check/1/run1/iter0/models/data2.traj
  #   - /localdata/stuff/fw/fast_mappo/results/football/check/1/run8/iter1/models/data.traj

policy:
  type: actor-critic
  args:
    hidden_dim: 64
    num_dense_layers: 2
    num_rnn_layers: 0
    std_type: fixed
    init_log_std: -0.5

environment:
  type: ma-mujoco
  args:
    base:
      env_args:
        scenario: Walker2d-v2
        agent_conf: 2x3
        agent_obsk: 2
        episode_limit: 1000
